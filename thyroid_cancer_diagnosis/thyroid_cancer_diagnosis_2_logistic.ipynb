{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Machine Learning for Thyroid Cancer Diagnosis.\n",
    "##  Part 2: Logistic regression\n",
    "**The project was done with Rajiv Krishnakumar and Raghu Mahajan.**\n",
    "\n",
    "The essential goal was to predict thyroid cancer given gene expressions. A key hope is to definitively predict benign samples; this helps to avoid unnecessary surgeries, which often turn out to be much more problematic to a patient's health, than the thyroid cancer itself.\n",
    "\n",
    "\n",
    "- The data used here is pre-normalized, to mean zero and standard deviation 1. \n",
    "- The essentials of the data set are 265 patients whose biopsies were inconclusive, each with 173 reported gene expression levels. \n",
    "- There were a further 102 patients with 'conclusive' biopsies - i.e. a human determination of benign vs. malignant, to give 367 total patients.\n",
    "\n",
    "Here is an abstract from our final report:\n",
    "\n",
    "*We investigate the use of high throughput gene expression data in the diagnosis of thyroid cancers. Using logistic regression and support vector machines (SVMs), we develop a classifier which gives similar performance (89% sensitivity and 80% specificity) to the currently best- known classifier, but uses significantly fewer features. We used two different techniques, principal components analysis and mutual information score, to select features. The results do not depend significantly on which method is used for feature selection.*\n",
    "\n",
    "The breakdown of topics covered in each notebook is as follows:\n",
    "1. Data visualization, including PCA and tSNE visualizations.\n",
    "2. Logistic regression, discussing the use of feature selection via mutual information vs. use of different regularizers.\n",
    "3. SVMs with and without box constraints, and also using different kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#As usual import some modules and import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import the data and look at it\n",
    "X = pd.read_csv(\"data/normalized_data_265.csv\", header =None)\n",
    "y = pd.read_csv(\"data/outcome_265.csv\", header = None)\n",
    "\n",
    "\n",
    "X_full = pd.read_csv(\"data/normalized_data_367.csv\", header =None)\n",
    "y_full = pd.read_csv(\"data/outcome_367.csv\", header = None)\n",
    "\n",
    "\n",
    "\n",
    "#Now turn these into numpy arrays to avoid problems with pandas dataframes\n",
    "X = X.as_matrix()\n",
    "y = y.as_matrix().reshape(len(y))\n",
    "X_full = X_full.as_matrix()\n",
    "y_full = y_full.as_matrix().reshape(len(y_full))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression \n",
    "\n",
    "A simple and straightforward first pass is always a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Results of train-test split-----\n",
      "(212, 173)\n",
      "(212,)\n",
      "(53, 173)\n",
      "(53,)\n",
      "\n",
      "-----Logistic Regression-----\n",
      "\n",
      "Training accuracy:  1.0\n",
      "Test set accuracy:  0.754716981132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "#First perform a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "print \"-----Results of train-test split-----\"\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Now perform the logistic regression\n",
    "logit_bare = LogisticRegression(random_state = 1, solver = 'liblinear')\n",
    "logit_bare.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print \"\"\n",
    "print \"-----Logistic Regression-----\"\n",
    "print \"\"\n",
    "print \"Training accuracy: \",logit_bare.score(X_train,y_train)\n",
    "print \"Test set accuracy: \",logit_bare.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ok - there is clearly some overfitting going on. We have 100% accuracy on the training set, but 70-80% accuracy on the test set. Let's break it down further by looking at commonly used metrics in biology:\n",
    "\n",
    "$ \\text{sensitivity} = \\frac{\\text{true_positive}}{\\text{actual_condition_positive}}$ and \n",
    "$ \\text{specificity} = \\frac{\\text{true_negative}}{\\text{ actual_condition_negative}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Specificity = 0.775\n",
      "Test Sensitivity = 0.692307692308\n",
      "Total Specificity = 0.95\n",
      "Total Sensitivity = 0.952941176471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_test = logit_bare.predict(X_test)\n",
    "y_pred_full = logit_bare.predict(X)\n",
    "\n",
    "confusion_test = confusion_matrix(y_test,y_pred_test)\n",
    "confusion_full = confusion_matrix(y,y_pred_full)\n",
    "\n",
    "print \"Test Specificity =\", confusion_test[0,0]/(1.0*(confusion_test[0,0] + confusion_test[0,1]))\n",
    "print \"Test Sensitivity =\", confusion_test[1,1]/(1.0*(confusion_test[1,0] + confusion_test[1,1]))\n",
    "\n",
    "print \"Total Specificity =\", confusion_full[0,0]/(1.0*(confusion_full[0,0] + confusion_full[0,1]))\n",
    "print \"Total Sensitivity =\", confusion_full[1,1]/(1.0*(confusion_full[1,0] + confusion_full[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
