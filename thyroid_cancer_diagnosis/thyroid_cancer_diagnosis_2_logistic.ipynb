{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Thyroid Cancer Diagnosis.\n",
    "##  Part 2: Logistic regression\n",
    "**The project was done with Rajiv Krishnakumar and Raghu Mahajan.**\n",
    "\n",
    "The essential goal was to predict thyroid cancer given gene expressions. A key hope is to definitively predict benign samples; this helps to avoid unnecessary surgeries, which often turn out to be much more problematic to a patient's health, than the thyroid cancer itself.\n",
    "\n",
    "\n",
    "- The data used here is pre-normalized, to mean zero and standard deviation 1. \n",
    "- The essentials of the data set are 265 patients whose biopsies were inconclusive, each with 173 reported gene expression levels. \n",
    "- There were a further 102 patients with 'conclusive' biopsies - i.e. a human determination of benign vs. malignant, to give 367 total patients.\n",
    "\n",
    "Here is an abstract from our final report:\n",
    "\n",
    "*We investigate the use of high throughput gene expression data in the diagnosis of thyroid cancers. Using logistic regression and support vector machines (SVMs), we develop a classifier which gives similar performance (89% sensitivity and 80% specificity) to the currently best- known classifier, but uses significantly fewer features. We used two different techniques, principal components analysis and mutual information score, to select features. The results do not depend significantly on which method is used for feature selection.*\n",
    "\n",
    "The breakdown of topics covered in each notebook is as follows:\n",
    "1. Data visualization, including PCA and tSNE visualizations.\n",
    "2. Logistic regression, with feature selection using different regularizers or different numbers of Principal components.\n",
    "3. SVMs with and without box constraints, and also using different kernel functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#As usual import some modules and import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#import the data and look at it\n",
    "X = pd.read_csv(\"data/normalized_data_265.csv\", header =None)\n",
    "y = pd.read_csv(\"data/outcome_265.csv\", header = None)\n",
    "\n",
    "\n",
    "X_full = pd.read_csv(\"data/normalized_data_367.csv\", header =None)\n",
    "y_full = pd.read_csv(\"data/outcome_367.csv\", header = None)\n",
    "\n",
    "\n",
    "\n",
    "#Now turn these into numpy arrays to avoid problems with pandas dataframes\n",
    "X = X.as_matrix()\n",
    "y = y.as_matrix().reshape(len(y))\n",
    "X_full = X_full.as_matrix()\n",
    "y_full = y_full.as_matrix().reshape(len(y_full))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression \n",
    "\n",
    "A simple and straightforward first pass is always a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Results of train-test split-----\n",
      "(212, 173)\n",
      "(212,)\n",
      "(53, 173)\n",
      "(53,)\n",
      "\n",
      "-----Logistic Regression-----\n",
      "\n",
      "Training accuracy:  1.0\n",
      "Test set accuracy:  0.849056603774\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "#First perform a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "print \"-----Results of train-test split-----\"\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Now perform the logistic regression\n",
    "logit_bare = LogisticRegression(random_state = 1, solver = 'liblinear')\n",
    "logit_bare.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print \"\"\n",
    "print \"-----Logistic Regression-----\"\n",
    "print \"\"\n",
    "print \"Training accuracy: \",logit_bare.score(X_train,y_train)\n",
    "print \"Test set accuracy: \",logit_bare.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ok - there is clearly some overfitting going on. We have 100% accuracy on the training set, but 70-80% accuracy on the test set. Let's break it down further by looking at commonly used metrics in biology:\n",
    "\n",
    "$ \\text{sensitivity} = \\frac{\\text{true_positive}}{\\text{actual_condition_positive}}$ and \n",
    "$ \\text{specificity} = \\frac{\\text{true_negative}}{\\text{ actual_condition_negative}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Specificity = 0.906976744186\n",
      "Test Sensitivity = 0.6\n",
      "Total Specificity = 0.977777777778\n",
      "Total Sensitivity = 0.952941176471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_test = logit_bare.predict(X_test)\n",
    "y_pred_full = logit_bare.predict(X)\n",
    "\n",
    "confusion_test = confusion_matrix(y_test,y_pred_test)\n",
    "confusion_full = confusion_matrix(y,y_pred_full)\n",
    "\n",
    "print \"Test Specificity =\", confusion_test[0,0]/(1.0*(confusion_test[0,0] + confusion_test[0,1]))\n",
    "print \"Test Sensitivity =\", confusion_test[1,1]/(1.0*(confusion_test[1,0] + confusion_test[1,1]))\n",
    "\n",
    "print \"Total Specificity =\", confusion_full[0,0]/(1.0*(confusion_full[0,0] + confusion_full[0,1]))\n",
    "print \"Total Sensitivity =\", confusion_full[1,1]/(1.0*(confusion_full[1,0] + confusion_full[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with L1 and L2 regulators.\n",
    "\n",
    "To avoid overfitting, we should introduce a regulator. Here we use GridSearchCV to loop over L1 and L2 type regulators, of different strengths, and find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Paramters:  {'penalty': 'l1', 'C': 1.0}\n",
      "\n",
      "-----Accuracies on Indeterminate samples (265 Patients)-----\n",
      "Final accuracy:  0.969811320755\n",
      "Specificity = 0.994444444444\n",
      "Sensitivity = 0.917647058824\n",
      "\n",
      "-----Accuracies on Full Data Set (367 Patients)-----\n",
      "Final accuracy:  0.931880108992\n",
      "Total Specificity, full data set = 1.0\n",
      "Total Sensitivity, full data set = 0.825174825175\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "parameters = {'penalty':('l1', 'l2'), 'C':np.logspace(-4, 4, 3)}\n",
    "\n",
    "estimator = GridSearchCV(logistic,parameters, cv = 5)\n",
    "estimator.fit(X,y)\n",
    "\n",
    "print \"Best Paramters: \", estimator.best_params_\n",
    "\n",
    "\n",
    "\n",
    "y_pred = estimator.best_estimator_.predict(X)\n",
    "confusion= confusion_matrix(y,y_pred)\n",
    "\n",
    "y_pred_full = estimator.best_estimator_.predict(X_full)\n",
    "confusion_full = confusion_matrix(y_full,y_pred_full)\n",
    "print \"\"\n",
    "print \"-----Accuracies on Indeterminate samples (265 Patients)-----\"\n",
    "print \"Final accuracy: \",estimator.best_estimator_.score(X,y)\n",
    "print \"Specificity =\", confusion[0,0]/(1.0*(confusion[0,0] + confusion[0,1]))\n",
    "print \"Sensitivity =\", confusion[1,1]/(1.0*(confusion[1,0] + confusion[1,1]))\n",
    "print \"\"\n",
    "print \"-----Accuracies on Full Data Set (367 Patients)-----\"\n",
    "print \"Final accuracy: \",estimator.best_estimator_.score(X_full,y_full)\n",
    "print \"Total Specificity, full data set =\", confusion_full[0,0]/(1.0*(confusion_full[0,0] + confusion_full[0,1]))\n",
    "print \"Total Sensitivity, full data set =\", confusion_full[1,1]/(1.0*(confusion_full[1,0] + confusion_full[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#An l1 regulator will lead to an automatic reduction \n",
    "# in the number of features used by the logistic regression.\n",
    "#Let us count the number of features used:\n",
    "np.count_nonzero(estimator.best_estimator_.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guarding against overfitting\n",
    "So the above numbers look really great, but just to check that we're not overfitting, let's do a train-test split once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Paramters:  {'penalty': 'l2', 'C': 1.0}\n",
      "Train accuracy:  0.77358490566\n",
      "Test accuracy:  0.77358490566\n",
      "Test Set Specificity = 0.825\n",
      "Test Set Sensitivity = 0.615384615385\n"
     ]
    }
   ],
   "source": [
    "#First perform a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#Train on the training set\n",
    "logistic = linear_model.LogisticRegression()\n",
    "parameters = {'penalty':('l1', 'l2'), 'C':np.logspace(-4, 4, 3)}\n",
    "\n",
    "estimator = GridSearchCV(logistic,parameters, cv = 8)\n",
    "estimator.fit(X_train,y_train)\n",
    "\n",
    "#Now use the test set\n",
    "print \"Best Paramters: \", estimator.best_params_\n",
    "print \"Train accuracy: \",estimator.best_estimator_.score(X_test,y_test)\n",
    "print \"Test accuracy: \",estimator.best_estimator_.score(X_test,y_test)\n",
    "\n",
    "\n",
    "y_pred_test = estimator.best_estimator_.predict(X_test)\n",
    "confusion_test = confusion_matrix(y_test,y_pred_test)\n",
    "\n",
    "print \"Test Set Specificity =\", confusion_test[0,0]/(1.0*(confusion_test[0,0] + confusion_test[0,1]))\n",
    "print \"Test Set Sensitivity =\", confusion_test[1,1]/(1.0*(confusion_test[1,0] + confusion_test[1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So maybe those numbers weren't that great after all!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA + Logistic regression pipeline\n",
    "We can adopt an *even more*  more principled approach by first doing PCA on the bare data, then logistic regression on this.\n",
    "\n",
    "I do this by following the python example of \"Pipeline\" - where we use GridSearchCV to apply cross validation and search over multiple values of pca components and logistic regression penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV4AAAD3CAYAAABVTzyIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VOXVwPHfzGQjYQnIvilqPcjiAhZEqTtu1Wrr2/oW\nl75a9S1ai62IFUWlqHWl7lawKFp361IXXOqCuLwaIiAoHkVFXAAJW0JC1pn3j3snTJJJMsTMzL2T\n8/188mHmztx7z0zCyZMzz31OIBKJYIwxJnWC6Q7AGGM6Gku8xhiTYpZ4jTEmxSzxGmNMilniNcaY\nFLPEa4wxKZaV7gBac/yFz0R+dejuHD12cLpDAaB793w2bapIdxiAxdIciyU+L8UC3oonGbH06tUl\n0Nxjvhjx1oXD6Q6hXlZWKN0hADB6dAG77+6NWMA77wtYLM3xUizgrXhSHYsvEm84bBd5NFZcXM6q\nVemOwhjTFr5IvHWWeI0xGcQXiTdslzUbYzKILxKvjXibGj26gF12SXcUxpi28EXijXjnszXPsBqv\nMf7li8RrI15jTCbxReK1WQ3GmEzii8RbZx+uNWE1XmP8yxeJN+yhCyi8wmq8xviXLxKv1XiN8bfr\nrruaTz/95AcfZ/7855g69Y/tEFF6+SLxWo3XGH9btOg92qtiGGh2BQT/8PwiOWAj3nhGjy4gGISi\nonRHYmItXlzM7Nl30r//AL788nMikTB/+MMU9t13dIv7ffTRcm655UaqqirJysrmvPMmM2rUfixd\nupg777yVqqoqsrOzOOusSYwdO47585/jjTdepaqqirVr19CnT19+8Ytf8q9/PcY333zNySdP5OST\nT2H+/Od45ZWXiETCbNq0gR49enLppVey0049Wb/+e2688a+sXbsGgKOO+ikTJ57G2rVrmDx5EuPG\nHchHHy2nrKyMs8+exOGHTwDg/vvnsmDB60QiYfr27c+FF17MTjv15Pzz/5cRI/Zi2bKlrFu3lr33\n3pdLL72SOXPuoqRkPX/5y2VcdtkMvv9+HfPmzSU3N5twGM49dzJ7771Pk/fkgQfu5cUXnycrK4uB\nAwcxbdoVAJSUlDB16gWsW7eWUCiLK6+8isGDd2n29dTV1fG3v93AsmVLyc7Opn//AUybdgV5eXks\nX/4hd911G7W11YTDEc488xzGjRvP/PnP8eabrxMIBPnmm9VkZ+dw2WUzGDJk13b5ObERr09Zjbeh\n0aMLknp/R6xY8RETJ57O3LkPctJJJzF37uwWn19bW8u0aVM488xzmDfvEaZOvZRbbrmJ0tItTJ/+\nZ/74x4u4776HmDbtSmbOnF6fWD78cCmXXTaDRx55io0bN/Dqq69w661/5/rrb2b27Dvrj798+YdM\nmXIJzz//PHvsMZSbb74RgBkzLmP06DHMm/cId955Dy+/PJ9XX30FgO+++5axYw9gzpx5TJr0e+66\n61bA+VP/889XMmfOPObOfZD99z+Aa6+dWX+u7777httvn828eY9QXFzEkiUfcM4559KzZy+uuOIq\n9txzOHfeeStTpvyZJ554grPO+h2LFy9q8p689dYCXnzxeWbPvo958x6hX78BPPnk4wCsWfMdF1xw\nEfPmPcLee+/Lww//s8XXs3z5MpYsKWbevIe555776d9/ACtXfkZZWRnXXDOD6dNn8uSTT/LXv87i\nhhv+yvffrwNgyZLF/OlPF3P//Y8ycuRePPzwA236eYjHRrzGtLO+ffux2267AzBs2DAef/xfLT7/\niy9WEgqF2H//AwAQGcq8eQ/z7rtvM3DgIIYOHQbAkCG7MnLkPixeXAzAnnsOo2fPXgD06zeAMWPG\nAjBgwEBqamqorKwEYMyYsQwYMBCA448/kTPPPIXKykqWLVvKzTc7CbqgoDPHHnsc7733DsOHjyA7\nO5tx4w4EYI89hlJWVgrAu+++zYoVH/Pb354KOIOiqqqq+tdy4IEHAZCfn8+AAQMpLS2tfyxaajji\niKO45JIpHHbYoYwYMYpTTvlNk/dk0aIiDj30CAoKOgPw+99fADiJf889h9O//wAAfvSjPXjzzddb\nfD2TJ19IKBTi7LN/w9ix4zjkkMMYOnQY7777Nhs2lDBt2oWEQkFqa+sIhUJ8/vln9d+Hnj171r8H\nb775eovfxx3hi8RrI17TmuLi8qTe3xG5ubn1twOBANDyz28olOU+b7svvvicSCRCpFFhNByuo7a2\nlqysLLKzs5scJ/7xty95GA6HCQZDcWcKhcNhamtrAcjK2n7sQCBQnzTD4TpOOeV0TjzxJMAZrUeT\nMiT22s8+exLHHXcCK1Ys4YUXnuXBB+9j7twHm8Qc+55s3bqVrVvL3Ni2vx4ntkiLr6egoDP33vsQ\ny5Z9yAcfFDF9+iX88pf/zYABA9lll125++576dWrC+vXl1FSUkL37t15+eX5TV5L4+/FD5H0UoOI\n9BaR1SKyh4jsJiILRWSBiNyR6DEs7zZl83gzx+DBOxMIBFi06H0AVD/hggvOZdiwEXz99Wo++eRj\nwEnGS5cuabVeDDRIEsXFRZSUlADwzDNPMn78QeTn5zN8+EiefPIxwElsL774Aj/+8dgm+8caM2Yc\nzz33DBUVzi+m2bPvZObMy1uNJxQKUVtbS11dHb/85c/Ytq2Ck08+mQsv/DNffbWqPuFH7bffGBYs\neI2KCmdx8rlz7+bRRx9q9vgtvZ533nmLyZMnMWLESM4442yOPvpYVq78lOHDR/LNN6tZunQxAJ99\npvz61z+npGR9q6/nh0rqiFdEsoC/A9Gl3WcB01R1oYjcJSInqOozrR3H5vE2VVxc7v6WTnck5ofK\nzs7m6qtv4JZbbuSOO24mOzuHa665gcLCQmbOvJZZs66nqqqSYDDIpZdewcCBg1i2bGmDYzT+pD92\ntNirV29mzryczZs3MGjQLkydeikAl19+FbNmXcvzz/+b2tpajjzyGI455jjWrl3TZAQedfzxJ1JS\nsp5zzjmDYDBAnz59ufTSK5ucs/H9n/zkEK644hIuvvgyJk++kBkzLiMvL5dwOMK0aVeQldUwFY0b\ndyBfffUlkyadCQQYMmRXLr74Mt5449Vm38fLL5/JrFnXNXk94XCY9957h9NPP5lOnfLp2rUrU6de\nRmFhIVdddT133HELt912EzU1dVx++Uz69Onb7DnaS6A9h8+NicjNwPPAJcAk4D+qOsh97GfABFU9\nv6VjHH/hM5GhgwuZOnFU0uLcEdE/SbzAYonPYtkuOvvhuuv+lvZYGvNSPMmIpaXWP0kb8YrI/wDf\nq+orIjLN3Rxb2igDuiVyLKvxGr976KEHeOWV+Q1GgZFIhEAgwK9/fRoTJhydxuhMqiWz1HAGEBaR\nCcDewP1Ar5jHuwCbEzmQrdXQlM3j9ZeJE09j4sTTUn7eY445jmOOOS7l5zUtS1riVdWDo7dF5DXg\nd8ANInKQqr4JHAO8lsixgsEgvXp1SU6gbeCFWFavjt5KfyxRXnhfoiyW+LwUC3grnlTGkurpZFOA\nOSKSDawAnkhkp6rq2oyuBbWVxRKfxRKfl2IBb8WTpBpvs4+lJPGq6mExdw/Z0f2txmuMySS+uGTY\nrlxryubxGuNfvki8NuJtytZqMMa/fJF4bcRrjMkkvki8ybzIwxhjUs0XiddGvE1ZjdcY//JF4rUa\nb1NW4zXGv3yReG3Ea4zJJL5IvGGr8RpjMogvEq+NeJuyGq8x/uWLxGs13qasxmuMf/ki8dqI1xiT\nSXyReCMRq/MaYzKH5xNvKOgsHG3lhoasxmuMf3k+8QYt8cZlNV5j/MvziTc64rU6rzEmUyS7y3AQ\nmAMIEMbpQpEDPAd86j7tLlV9vLljRBOvrddgjMkUyV4I/XggoqrjReRg4BrgWeAmVf1bIgcIBp1B\nuY14G7Kea8b4V1JLDar6DHCOe3cXYBMwGjhORBaIyD0iUtDSMUIhq/HGYzVeY/wr6TVeVQ2LyH3A\nLcCDwHvAFLcZ5hfAlS3tbzVeY0ymSVXPtf8Rkd7A+8A4VV3jPvQUcGtL+0YTb2H3Anr1yE9qnInq\nqJ1RW2OxxGexNM9L8WRMl2ERORUYqKrXApU4H7A9KSJ/UNUi4HCguKVjhNwa7/qSMoJ1dckMNyFe\n6Yzq1HiDFBWlPxbwzvsCFktzvBQLeCueTOsy/CRwr4gscM81GfgauF1EqoG1bK8Bx2XzeOMrLi53\nf1jSHYkxZkclNfGqagVwcpyHxid6jOiHa1bjNcZkCs9fQBEM2IjXGJNZPJ94bcQbn63VYIx/eT/x\nWo03LpvHa4x/+SDxOiHaspDGmEzh+cRrsxqMMZnG84nXrlyLz2q8xviXbxKvjXgbshqvMf7l/cQb\nstXJjDGZJeELKERkIjAcuBr4L1W9P2lRxbARrzEm0yQ04hWRa4FjgV/gJOszROSmZAYWFbQab1xW\n4zXGvxItNRwFnAZUqmopMAE4JmlRxaif1WDTyRqwGq8x/pVo4g27/0azX27MtqSyWQ3GmEyTaOJ9\nDHgU6CEiFwALgYeSFlUMq/EaYzJNQolXVa8D/gE8DgwGpqvqNckMLCpkPdfishqvMf6V0KwGEekP\nHKaqF4nIEGCGiCxS1XWt7Bevy3AVcJ97f7mqntfSMep7rlmNtwFbj9cY/0q01PAgTn80gO9wSg0P\nJLBffZdhYDpOl+FZwDS351pQRE5oMUArNRhjMkyiibeHqt4NoKpVqjoH6NnaTo26DO+M02V4lKou\ndLfNB45o6Rj24ZoxJtMkmni3iUj99DERORwoT2THmC7Dt+J8IBeIebgM6NbS/vWrk1nibcBqvMb4\nV6JXrv0O+KeIPICTOFfjzOtNSEyX4SKgU8xDXYDNLe1rsxrisxqvMf6VUOJV1SXACBHZCahxL6Jo\nVZwuw3XAIhE5WFUX4FyE8VpLx4h+uJbXKcczraC9EgdYLM2xWOLzUizgrXg8195dRPYFpgE9gICI\nAKCqh7Wya+Muw38APgHuEZFsYAXwREsHiH64VlZW6YlW0JnekrqtLJb4LJbmeSker7Z3vx+4G1jO\n9qvXWtVCl+FDEj1GKGAfrsUzenQBwSAUFaU7EmPMjko08Vao6u1JjaQZQZvHG5fVeI3xr0QT70si\ncj7wEk6tFgBVXZ2UqGLYlWvGmEyTaOKNzmD4U8y2CLBr+4bTlM1qMMZkmkRnNQxJdiDNscQbn9V4\njfGvRGc1CHAu0BlnHm8IGKKqByUxNiDmyjWr8TZgNV5j/CvRK9cexbnQYV9gCdAbZ4ZD0gVDduWa\nMSazJJp4g6p6BfAi8AFwIjA2aVHFsLUajDGZJtHEWyEiucCnwGhVrQLykhfWdlbjjc/WajDGvxKd\n1fBP4FngFOBdETka+DZpUcWwxBuf1XiN8a9EO1DcDpykqutxrjqbjVNuSDrrMmyMyTQtjnhF5BxV\nnS0il7v3Yx8eCfwlibEBtiykMSbztDbiDcT8G+8r6WzEG5/VeI3xrxZHvNGuE8AuqnpGCuJpwnqu\nxWc1XmP8K9FZDSNEpHNSI2mGTSczxmSaRGc1hIHVIqLAtujGBNbj/cFsVoMxJtMkmnin7uiBRSQL\nmAvsAuQAVwNfA8/hzAcGuEtVH2/pOPbhWny2VoMx/pXodLIFQCnOyDfi7rdbK7udCpS46zkcA9wO\njAJuUtXD3K8Wky7EfLhmNd4GiovLWbUq3VEYY9oi0UVy5gEH4LT+WQHsA7yNM6JtzmNANLEGgRpg\nNDBURE4EPgMmq2qL3YrrP1yzEa8xJkMk+uHaQcAwnER6Ds46DTkt7aCqFapaLiJd3P0uA94Hpqjq\nwcAXwJWtndg+XDPGZJpEa7zfqWqNiKwA9lLVR9yE2iIRGYTT8PJ2d59uqrrFffgp4NbWjhF0e64F\ngwHPdCT1QhzRObyrVqU/ligvvC9RFkt8XooFvBWP57oMA9+KyCXAf4Dr3SvYWpxeJiJ9cFoFnaeq\nr7ubXxKR36vqIuBwoLi1E4fcZSGrq+s80ZHUK51Ri4q8EwtYLM2xWJrnpXi82mX4t8BPVbVIRJ4E\nfg1MamWfS4BCYLp7yXEE+CNws4hUA2txyhYtslKDMSbTJJp4Z+KsUIaq3gbc1toOqnoBcEGch8Yn\nHB02j9cYk3kSTbyf4YxUewAPAf9U1VVJiyqGrdUQn83jNca/Ep3He4eqjgeOxmnv/rSIvJXUyFz1\nF1DYPN4GbB6vMf6V6HQyRKQbcARwJM5I+aVkBRUrOo/XRrzGmEyR6AUUz+I0unwSmK6q7yU1qhhW\n4zXGZJpER7yzcZaG/EPjpCsirc5M+CEs8cZn6/Ea41+J1nifVdXaZh7+XTvG04St1RCf1XiN8a+E\na7wtSGoniugFFDbiNcZkivZIvEnNiNFLhi3xGmMyRXsk3qSKvXItYuWGelbjNca/PJ94g8FAfS3D\n8u52VuM1xr/aI/FubodjtMiuXjPGZJIW5/G6i9s0S1X/kqq+a3XhiNV5jTEZobURb8D9GguchNP6\npxr4KTA8uaFtZyPepqzGa4x/tTjiVdUZACLyNjBOVSvc+zcDr7e0b3uqv4jCirz1iovL3TVE0x2J\nMWZHJbo6WS8aThvLxum/1qxmugx/DNyHM3JerqrnJXLyoF29ZozJIIl+uDYHWCQiN4jITcAi4OZW\n9ontMnw0TpfhWcA0t+daUEROSChIKzUYYzJIopcM3wCcjtM14lvgV6p6Vyu7PQZMd2+HgFpglKou\ndLfNx1ntrFW2XkNTVuM1xr92ZDqZ4JQX7gb2bu3JcboMX0rDy4vLgG4JBRmw9Roas3m8xvhXQolX\nRK4FjgV+gTN6PcMtObS23yDgNWCeqj6CU9uN6kKCc4CtxmuMySSJfrh2FDAK+EBVS0VkAvAhcGFz\nOzTTZXixiBykqm8Cx+Ak5VblZIcA6NatkyfaQXshhiiLJT6LJT4vxQLeiseL7d2jI9XokDOXhqPX\neOJ1GZ4M3CYi2cAK4IlETh5do6FkQzmdQkldDK1VXmlJ7fRcC1JUlP5YwDvvC1gszfFSLOCteLza\n3v0x4FGgh4hcAJyG0/SyWS10GT4kwXPWC9kKZU3YPF5j/CvRWQ3XAf/A+ZBsMHCFql6TzMBi5eU6\nvx82lFam6pTGGJM0OzKr4Rvg38DTQKmIHJSckJoaPsS5VmPxZza8M8b4X6LNLu8Ajgc+j9kcAZK+\nQA7AqB/15Kk3v2Dpyg3UhcP1Ld87MqfGC0VF6Y7EGLOjEq3xHgmIqm5LZjDN6d+zgD7dO7Fu0zY+\n/XoLe+7cPR1heIrVeI3xr0SHjl+Q5N5qLQkEAozaoxcAiz+1TGOM8bdER7wbgY9F5B2g/hMuVT0z\nKVHFMWqPXsx/bzWLPyth4oQ9UnVaY4xpd4km3hfdr7QZ0q8rAZyZDeFwpP5qto7KarzG+FdrHSj6\nqupaUrj2bnOCwQB5uVlsq6qloqqWzp2y0x1SWlmN1xj/am3Eew9wHLAAZxZD7DAzAuyapLjiKsiz\nxGuM8b/WOlAc5/47JDXhtCzfvZCiorIG6JTeYIwxpo0SnccrwLlAZ5xRbwgY4i5ynjL5edHEW5vK\n03qS1XiN8a9Ep5M9irOE477AEqA3sDxZQTUnP88pL1jitfV4jfGzRBNvUFWvwJnZ8AFwIk7n4ZSq\nH/FWWeI1xvhXoom3QkRygU+B0apaBeQlL6z4ttd4LfEaY/wr0cT7T+BZ4HngfBGZj9N7LaWiI97y\nyppUn9pzrOeaMf6V6LKQtwMnqep6nPV0Z+OUG1olImNF5HX39j4i8o2IvOZ+/XJHgi2I1nit1GA1\nXmN8rLULKC5vdD/27kjgL63sfxHOoulb3U2jgZtU9W87HClWajDGZIbWppP90OtyVwI/Bx5w748G\n9hCRE4HPgMmqWp7owWw6mTEmE7R2AcWM6G0R6Q2MB2qBhaq6qbWDq+pTIrJzzKb3gDmqulhEpgFX\nAhclGuz2xGs1XpvHa4x/JXoBxSnATcBbOBdP3CUiZ6vqCzt4vqdVdYt7+yng1kR2ijaN21bn9Fyr\nqg2ntTupFzqjrl4dvZX+WKK88L5EWSzxeSkW8FY8XuwyPB1nGtm3AO4o9llgRxPvSyLye1VdBBwO\nFCeyU7T7Z2VFNQCl5dVp606a6Z1R28piic9iaZ6X4vFql+FSYE30jqp+JSLVbYhlEk5792pgLXDO\njuxcYDVeY0wGSDTxLgNeEJF7cWq8vwLWiMjpAKp6f3M7qupXwAHu7cU4deI2yc4KkhUKUFsXprqm\njpzsUFsP5XtW4zXGvxK+ZBhnxHs0zjKRFUAJcCjOvN6UCAQC29dr6OBzeW0erzH+leiI91JV/S52\ng4iMUdX3kxBTi/Jzsygtr6a8spbCzrmpPr0xxvxgiY5434teZSYi2SJyHfBY8sJqXrTOu83qvMYY\nn0o08R6Ks0bDo8AinFXIRyYtqhZ0ql+hrGPP5bW1Gozxr0QT72rgDZwPxroDr6lqWuaBRNdrKO/g\nI16r8RrjX4km3uXAIGBPYAIwVUSeTFpULbD1Gowxfpdo4p0C/Ae4GPgapwnmu8kKqiV22bAxxu8S\nTbwHAMcAv8CZCXEa0C9ZQbXEulA4rMZrjH8lmniPwkm2lapailNuODppUbUgWmqwGq/VeI3xq0QT\nb9j9N+L+mxuzLaWiH67ZdDJjjF8lmngfw+k03ENELgDeBB5KWlQt6JLvJN7NW6vScXpjjPnBEm39\ncx3wD+BxYDBwhapek8zAmtN3pwIAvttQQSQSaeXZmctqvMb4V6KXDKOqLwEvJTGWhHTNz6YgL4vy\nylo2b62me5eOedlwcXG5u5RduiMxxuyoREsNnhEIBOjX0xn1rtmQcNcgY4zxjKQn3kZdhncTkYUi\nskBE7mjrMftHyw0llniNMf6T1MTrdhmegzMLAmAWME1VDwaCInJCW47bf6d8wKnzdlRW4zXGv5I9\n4o12GY4araoL3dvzgSPactD+0VJDBx7x2jxeY/wrqYlXVZ/C6VgRFdsuvgzo1pbj9quf2dBxE68x\nxr9S/eFa7EUXXYDNbTlIj6655OaEKKuooayiLa3fjDEmfRKeTtZOPhCRg1T1TZy1H15LZKd43ToH\n9enCyq83s60Odk1xi2gvtKSO1ndXrUp/LFFeeF+iLJb4vBQLeCseL7Z3by9TgDkikg2sAJ5IZKd4\nbZd7d8tj5dfw8cr19O6S075RtsArLamLirwTC1gszbFYmueleLza3r3NGnUZ/ox2ao7Zz53ZsHZj\nx53ZYIzxJ99dQBHVp7uTeL/ftC3NkRhjzI7xbeLt3b0TAOs2dcwRr83jNca/fJ9412/eRjjc8RbL\nsXm8xviXbxNvXk4W3QpyqK2LsLG0Mt3hGGNMwnybeAH61JcbrM5rjPEPXyfe3j2iH7B1vDqv1XiN\n8S9fJ96OPOK1Gq8x/uXzxGtTyowx/uPrxNvRp5QZY/wpIxJvR5xSZjVeY/zL14k3LyeLbp075pQy\nq/Ea41++TrwA/dyZDd924EXRjTH+4vvEO7iPswLQqrXeWOXIGGNa4/vEO6RfVwC+XFOa5khSy2q8\nxvhXBiRed8S7ppRIpON8wGY1XmP8K9ULoQMgIsXAFvful6r627Yeq1dhJwrysiitqGFjaRU7dctr\nnyCNMSZJUp54RSQXQFUPa4/jBQIBhvTryvIvN/LlmlJLvMYYz0tHqWFvoEBEXhKR/4jI2B96wF06\nYJ3XarzG+Fc6Em8FcIOqHgVMAh4UkR8UR7TO25ESr9V4jfGvdNR4PwVWgtODTUQ2AP2Ab5vbobXu\nn/vlZsO/lvHVuq0Uds8nOyvUnvHucDypZLHEZ7HE56VYwFvxZHKXYYAzgZHAeSLSH+gCrGlph0S6\nfw7u05nV67Yyf+EXjBvRt10CjSfTO6O2lcUSn8XSPC/Fk+ouw+koNfwD6CYiC4GHgTNVNfxDD3rY\nqIEAvPbBNz/0UL5gNV5j/CvlI15VrQFObe/jjh3Wh8deW8nn35Xy1doydu7rnT9hkqG4uNz9LZ3u\nSIwxO8r3F1BE5WaHGL9XPwD+s+jrNEdjjDHNy5jEC3DYqAEEAwHe/WgdazbYojnGGG/KqMTbu3s+\nB+3dj3Akwr8WfJHucJLKarzG+FdGJV6An40fQk52kA8+Xc/Kb7e0voNP2TxeY/wr4xJvYedcjvzx\nIACee2dVeoMxxpg4Mi7xAkzYbxA5WUE+/HwD36zfmu5wjDGmgYxMvF3yc+pnOLz03uo0R5McVuM1\nxr8yMvECHDlmMIEA/N/H69DVm9IdTruzGq8x/pWW9XhToXdhJw4Y0Ze3l63l+ocXs/+wPnTvkseP\nh/bO+IsrjDHelrGJF+A3Rw+lsHMuL7z7Fe9+tA6Al4u+5ncnDGfUHr3SHJ0xpqPK6MSbFQpy0sG7\nsZ/0ZuW3W/j82y3838fruOOpZRwwvC+j9uhFfl4WvQo70aOrvxZQHz26gGAQiorSHYkxZkdldOKN\n2rlvF3bu24XDRg2gT498nnnrS95evpa3l68FIADsvXtPhg/pQZf8bH40sJDuXXLTG3QrbK0GY/yr\nQyTeqEAgwAnjhzB2WB/+76O1rPx2C9W1YVatKWXJyhKWrCypf+7gPp3ZZ/ee/GhQIV06ZdO5UzYF\nnbLTGL0xJlN0qMQb1bdHPif+ZNf6+6Xl1byzfC3rN29jQ2kln6zexOp1W1m9rukc4G6dc+jVrRO9\nuzvliU45IXbqlseg3p3Jz80iFAqSn5dFMBBI5UsyxvhIOppdBoA7cXqvVQJnqWpaF1boWpDD0WMH\n19+vqa1jxVebWfp5Cd+tL6e8sobyylrKKmrYsrWaLVurW7wcORQMUNg5h8LOueTnZZObHaSwSy49\nu+aRl5tFXk6InOwQudmhmNtB8nKyyM0OkhUKEmglcVuN1xj/SseI90QgV1UPcBtdznK3eUZ2Voi9\ndtuJvXbbqcH2cCRCKCebj1euZ92mCrZsrWZbVS3rNlbwbUk51bVhamrDbKuqZUNpFRtKq9p0/kCA\nhsk5O0ROjnM7+jX5mhCFXfN4emEduY0ea3DfvZ0VCpAVCpIVChAKBW1EbkwapSPxjgdeBFDV90Rk\nvzTE0CZarwXRAAAJgklEQVTBQICehZ3Yc+fu7Llz92afV11Tx+byajaXVbGtqpbK6jo2llWysbSK\nquo6qmrcr9jbMfdr6yJsq6pjW1Vd0l5LKBggFAqQFQySlRWsT8zZoWB9gs4KuY8FAwSDAXcfJ2mH\nottCAUKBAJ0751JVWUMoFKh/PPa5oZD7/EDM7WBw+3Fivpp7LBAMEAw434dg0DlPIED9bWc71NaF\nCUci9svFeFY6Em9XIPbv9FoRCbZH+x+vyMkO0buwE70LO7Vp/7pwmKrqcPwEHXM7KzuLjZsrGm0P\nU1VdS1VNmMrqOqpr6qiqraOuLkJtXdj9ilAXdr6qCUPbBua+EE3GwUDDxB2oT94NE3fT5zgfygYD\nzr+BAARw/3W3Q8PnEN0fyMvLprq61tkX568ZYm432F5/XOccznFit8fs5z7eMJ7mt0OAgoIctlVU\nx7wO52Cxr6H+GLjnbhQr9eevD5r6X2+B7f80LpXFPid6/K5dN1FWVrn9OTGPNfqnwX6Nf59G35eG\n27bvGIhzLPeR+tuF68vZsmVbg/0avNbG+9XH1PjY29+7lnqupSPxluI0uIzKqKTbHkLBIPl5zod0\nzXFqvEGKina8QV8k4ibeugg19cnYSci1tWFq6sJNHguHnV8I4fD2pB2O+TevUw6lZdsaPN74Oc62\n+Mdo9Tl1EcIR9yscIRyBcDhCJOLejkSIhKPPcR4DZ3u4DiDSxu+GMW0zfvTgZh9LR+J9GzgOeEJE\n9geWtfL8gJdaQIM3WlKvrl/7J/2xGGN2TDoS71PABBF5271/RhpiMMaYtAlEIvYnmDHGpFLGLgtp\njDFeZYnXGGNSzBKvMcakmCVeY4xJMc8ukpPuNR1EJAuYC+wC5ABXAx8D9wFhYLmqnpeqeNyYegOL\ngCOAujTH8mfgZ0A2zvfpzXTE436f5uF8n2qBs0nDe+Ne/n6tqh4qIrvFO7+InA2cA9QAV6vq8ymI\nZR/gVpz3pgo4XVXXpyOWmG0Tgd+r6gHu/XS8L72AOUAhEMJ5X75MVSxeHvHWr+kAXIKzpkMqnQqU\nqOpBwNHA7W4M01T1YCAoIiekKhg3wfwdqHA3pTOWg4Fx7vfmEGBwGuM5Fgip6oHATOCaVMciIhfh\n/CeOLuLc5Pwi0gc4HxiH8/P0VxFp93VG48RyM3Ceqh6GM5Xz4jTGgojsC5wZcz9dsVwP/FNVDwGm\nA0NTFQt4O/E2WNMBSPWaDo/hfEPA+Y1YC4xS1YXutvk4I89UuRG4C/gO5wrFdMZyFLBcRJ4G/g08\nl8Z4PgWy3L+QuuGMVFIdy0rg5zH3Rzc6/wRgDPCWqtaqainwGbBXCmI5WVWjFyll4fz1mJZYRGQn\n4Cpgcsxz0vW+HAgMFJFXgInAGymMxdOJN+6aDqk6uapWqGq5iHQBHgcuhQaXhJfh/EdPOhH5H+B7\nVX0lJobY9yJlsbh6AqOB/wImAQ+mMZ6twBDgE+BunD+rU/p9UtWncH4xRzU+f1ecSwxjf563JiOu\nxrGo6joAETkAOA/4G03/byU9Fvf/7j3An4DymKelPBbXLsBGVZ0AfA38OVWxgLcTb9rXdBCRQcBr\nwDxVfQSnZhfVBdicolDOwLna73Wcmvf9QGy3zlTGArABeMkdGXyKM4qK/QFNZTx/BF5UVWH7e5OT\nplii4v2clOL8x268PelE5GScOvyxqrohTbGMAnbH+avtYWCYiMxKUyzg/Aw/695+Fucv6i2pisXL\nifdtnPodCa7p0K7ces9LwFRVneduXiwiB7m3jwEWxt25nanqwap6qPsBxRLgNGB+OmJxvYVTA0NE\n+gMFwKtu7TfV8Wxk+yhlM86f04vTFEvUB3G+N0XAeBHJEZFuwFBgebIDEZFTcUa6h6jqV+7m91Mc\nS0BVF6nqSLfW/N/Ax6r6pzTEErUQN78AB7nnTNn3yLOzGkj/mg6X4HziOV1ELsdZ3moycJtbcF8B\nPJHimGJNAeakIxZVfV5EfiIi7+P8WT0JWAXck4Z4bgbmisibODMs/gwUpymWqCbfG1WNiMitOL+0\nAjgfvlUnMwj3z/tbgK+Ap0QkAixQ1RkpjqXZdQlUdV2q3xfXFJyfkUk4v7gnquqWVMViazUYY0yK\nebnUYIwxGckSrzHGpJglXmOMSTFLvMYYk2KWeI0xJsUs8RpjTIpZ4jUmRUTkxyJybbrjMOlnideY\n1BkG9E53ECb97AIKk1LupbzTcJa33BP4EOeqodpmnj8RZ4GiMM5axGfhrMUwB2dthjrgJlV9QER+\nA/wUGOB+3YKzZOVhQAnO5bv9cK7N/xz4Ec4Vd6eq6mYROQ5nackA8AXwv+7atV8CD+CsypaPs3br\nYnfd3buAHu7rOV9Vl4rIvThXQ41245gBPO2+1gLgJpwV3WbjrHxXCZyhqp+3/Z01fmIjXpMO44Bz\nVXUosDNOQmvCXQdiFnCEqo7E+Xn9KXAlzlrJI4HDgStFZIS724+BI3Guv78JeF5V98ZJptHzjABm\nqeoInFXNrnQXxv478DNV3Qd4B2cN5qj1qjoWZwW0ae62ecBFqrof8L/AozHPH6iqP8FZLP4mVd0C\nXA78W1X/irO4z42qOga4Ddg/4XfP+J4lXpMOy1V1jXt7Bc6IMZ5xOOujrgFQ1d+o6r9xRrD/cLdt\nwBlNHuLu87aqlqvqapw1Al5zt38FdHdva8x6ufNwkvcY4D1V/drdPtvdHvVSNHagh4gU4CT5e0Vk\nMfAQkC8i0XO87J5oecx5Yz0P3CEi9+CsIfxQM++ByUBeXiTHZK7KmNsRGq5fG6sm9jER6enebPz8\nINt/lhssatLMUqJ1jfaNnifQaHvs/49ozNF4Q8A2VR0VE19/Vd0kIrHPj0tV/yUi7wDHARfgrJR1\nTkv7mMxhI17jZUXAGLfXHDiLeP8MZxR7FtQn4xNwOgg01lxCFxGJdhY4A3gBZ3nCsSIy2N1+DttH\ny01EOxSIyCnuASfg9J2LJxpHLW4yF5FHgLGqOgen08m+zZ3LZB5LvCbdWloycA3OUpwvi8iHOB9g\n3YvzAVgPd9sbwFWquqSVY8fe3gjMEJHlOAvKX62q3+Mk26dFZBlOjXhSKzGeCpwlIktxmqH+qpnn\nR++/D+wvIte4z58mIsXADTg1X9NB2KwG06GIyM7AG6o6JN2xmI7LarwmrUQkD3iXhqPEgHv/clV9\nLgmntdGGSSsb8RpjTIpZjdcYY1LMEq8xxqSYJV5jjEkxS7zGGJNilniNMSbFLPEaY0yK/T+RoGkP\nPK9djwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11eb5e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Paramters:  {'logistic__C': 1.0, 'pca__n_components': 40}\n",
      "Final accuracy:  0.88679245283\n",
      "Total Specificity = 0.95\n",
      "Total Sensitivity = 0.752941176471\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition, linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "pipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(X)\n",
    "\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.clf()\n",
    "plt.axes([.2, .2, .7, .7])\n",
    "plt.plot(pca.explained_variance_, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('explained_variance')\n",
    "\n",
    "###############################################################################\n",
    "# Prediction\n",
    "\n",
    "n_components = [20, 40, 60, 80, 100, 120, 140]\n",
    "Cs = np.logspace(-4, 4, 3)\n",
    "\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "\n",
    "estimator = GridSearchCV(pipe,\n",
    "                         dict(pca__n_components=n_components,\n",
    "                              logistic__C=Cs))\n",
    "estimator.fit(X, y)\n",
    "\n",
    "plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n",
    "            linestyle=':', label='n_components chosen')\n",
    "plt.legend(prop=dict(size=12))\n",
    "plt.show()\n",
    "\n",
    "print \"Best Paramters: \", estimator.best_params_\n",
    "print \"Final accuracy: \",estimator.best_estimator_.score(X,y)\n",
    "\n",
    "\n",
    "y_pred_full = estimator.best_estimator_.predict(X)\n",
    "confusion_full = confusion_matrix(y,y_pred_full)\n",
    "\n",
    "print \"Total Specificity =\", confusion_full[0,0]/(1.0*(confusion_full[0,0] + confusion_full[0,1]))\n",
    "print \"Total Sensitivity =\", confusion_full[1,1]/(1.0*(confusion_full[1,0] + confusion_full[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
